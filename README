HIGH LEVEL DESCRIPTION

project setup & build:

1. Get and install Leiningen http://github.com/technomancy/leiningen
2. In cascading-clojure home run: lein deps; lein install

This should download all dependencies, and install cascading-clojure in your local repo.

The goal of this wrapper is to be able to write as much application code in clojure as you can, and treat cascading as 
almost like an HOF, and one of the restrictions is where you have to follow certain conventions in terms of arity of 
fns.

Right now, it works on rows, where each row can be thought of as an item in the "collection". (from an 
HOF perspective)
A row depends on your input format or scheme, (in cascading parlance) for example, if you use the default textline, each row is a line of text from the input files.

The "framework" allows you to define read and write functions. Examples are identity, read-string, fns for reading json, 
etc.
Mostly, this wrapper has been used to convert and persist data as clojure data structures.

You can also define input fields and output fields.


DEFINING WORKFLOWS

sample code at: http://gist.github.com/183376

there are currently a few supported cascading constructs...
each, every, and groupby.

Each of these take in functions that's called per row, or in everygroup's case,
per group.

Workflows are defined as functions that return maps, these are called by the
internal wrapper code when you specify which workflow/function to call in the
commandline when you tell hadoop to run a job. (more on this later)

These workflow maps in general have a function for the operation, a reader
(which converts a row into something meaningful for your program), and a writer,
 (usually just print-string/pr-str). There are defaults defined for each of
these... of the cascading constructs take in additional fns. (for example,
groupby takes in a function to extract a group key out of the line of input)

The readers and writers are called per field of the output rows.

EACH

Each is like map in clojure, it takes in a function with an arity of the number 
of fields you specify, which defaults to 1, and calls it over each row.

You can multiple results per row that you process, and each row has to 
correspond to the input fields that you specify.


The output shape has to be a seq of seqs.
The inner seqs are rows, and since each call to Each can return multiple rows,
we need the outer seq.


The defaults are
:using identity :reader read-string :writer pr-str :inputFields ["line"]
:outputFields ["data"]


GROUPBY

GroupBy groups rows together, it is typically followed by an everyGroup. It
groups things by Fields/FIRST. The difference with each is that it takes in a
groupkey extractor function. This groupkey extractor has an arity of the num of
input fields. The return value is jsut anything that implements comparable.

Everything else is pretty similar to each, just don't forget that the first
field of the outputFields will be used as the key.


defaults:
:using identity :reader read-string :writer pr-str :groupby (fn [x] 1)
:inputFields ["line"] :outputFields ["key", "clojurecode"]



EVERYGROUP

This has to be preceded by a groupBy, as per cascading rules. 
(http://www.cascading.org/javadoc/cascading/pipe/Every.html)
EveryGroup is similar to reduce/fold, with an "init" function that will return
the initial value. This initial value has to be a seq that matches the
outputFields count. (since we're "collecting" these values per row we process)

the using function has to take in the accumulated row so far, and should output
a row. These rows should match the number of outputFields.


defaults:
:using (fn [acc next-line] [(str (first acc) next-line)]) :reader read-string
:writer pr-str :init (fn [] [""]) :inputFields ["line"] :outputFields ["data"]


A MORE FULL-FLEDGED EXAMPLE

..........

PACKAGING AND MAIN CLASS

Packaging-wise, the final jar needs to have cascading and it's dependencies, and
also clojure jars in the lib folder, with the rest of your dependencies. (it's a
hadoop thing) There should also be a main class, which should look like this:

 (ns mycompany.filename
  (:require [org.parsimonygroup.cascading :as c])
  (:require [org.parsimonygroup.makemain-utils :as m])
  (:gen-class))
  
(defn- -main [& args]
  (let [opts (assoc (m/parseArgs args) :main-class (class -main))]
    (if (:join opts) 
      (c/cascading-join opts)
      (c/cascading opts))))


Cascading-clojure uses apache commons cli to parse commandline arguments. A 
sample command looks like this: 
       hadoop jar build/jobs.jar -in \
         file:///Users/Admin/projects/githubcontestcode/calcs/splitupusers/ -out
         file:///Users/Admin/projects/githubcontestcode/out/ -wf /
         timwee.remote-jobs/user-covisit-wf

-in and -out are self-explanatory, -wf is the fully-qualified name of the
function that defines your workflow map.

GOTCHAS

cascading textline's fields are "offset" and "line". you have to name the
inputfields properly. Also when specifying multiple steps in the workflow, your
input and output fields have to line up for consecutive steps.
